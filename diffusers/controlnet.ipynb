{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_lkdXOQmjnV"
      },
      "source": [
        "ver since Stable Diffusion took the world by storm, people have been looking for ways to have more control over the results of the generation process. ControlNet provides a minimal interface allowing users to customize the generation process up to a great extent. With [ControlNet](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet), users can easily condition the generation with different spatial contexts such as a depth map, a segmentation map, a scribble, keypoints, and so on!\n",
        "\n",
        "We can turn a cartoon drawing into a realistic photo with incredible coherence.\n",
        "\n",
        "<table>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <th>Realistic Lofi Girl</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/lofi.jpg\" width=300 /></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Or even  use it as your interior designer.\n",
        "\n",
        "<table>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <th>Before</th>\n",
        "    <th>After</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/house_depth.png\" width=300/></td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/house_after.jpeg\" width=300/></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "You can turn your sketch scribble into an artistic drawing.\n",
        "\n",
        "<table>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <th>Before</th>\n",
        "    <th>After</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/drawing_before.png\" width=300/></td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/drawing_after.jpeg\" width=300/></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Also, make some of the famous logos coming to life.\n",
        "\n",
        "<table>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <th>Before</th>\n",
        "    <th>After</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/starbucks_logo.jpeg\" width=300/></td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/YiYiXu/controlnet-testing/resolve/main/starbucks_after.png\" width=300/></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "With ControlNet, the sky is the limit ğŸŒ \n",
        "\n",
        "In this notebook, we first introduce the [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet) and then show how it can be applied for various control conditionings. Letâ€™s get controlling!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcIUvHFgm5Km"
      },
      "source": [
        "## ControlNet: TL;DR\n",
        "\n",
        "ControlNet was introduced in [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543) by Lvmin Zhang and Maneesh Agrawala.\n",
        "It introduces a framework that allows for supporting various spatial contexts that can serve as additional conditionings to Diffusion models such as Stable Diffusion.\n",
        "\n",
        "Training ControlNet is comprised of the following steps:\n",
        "\n",
        "1. Cloning the pre-trained parameters of a Diffusion model, such as Stable Diffusion's latent UNet, (referred to as â€œtrainable copyâ€) while also maintaining the pre-trained parameters separately (â€locked copyâ€). It is done so that the locked parameter copy can preserve the vast knowledge learned from a large dataset, whereas the trainable copy is employed to learn task-specific aspects.\n",
        "2. The trainable and locked copies of the parameters are connected via â€œzero convolutionâ€ layers (see [here](https://github.com/lllyasviel/ControlNet#controlnet) for more information) which are optimized as a part of the ControlNet framework. This is a training trick to preserve the semantics already learned by frozen model as the new conditions are trained.\n",
        "\n",
        "Pictorially, training a ControlNet looks like so:\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://github.com/lllyasviel/ControlNet/raw/main/github_page/sd.png\" alt=\"controlnet-structure\"><br>\n",
        "    <em>The diagram is taken from <a href=https://github.com/lllyasviel/ControlNet/blob/main/github_page/sd.png>here</a>.</em>\n",
        "</p>\n",
        "\n",
        "A sample from the training set for ControlNet-like training looks like this (additional conditioning is via edge maps):\n",
        "\n",
        "<table>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <th>Prompt</th>\n",
        "    <th>Original Image</th>\n",
        "    <th>Conditioning</th>\n",
        "</tr>\n",
        "<tr style=\"text-align: center;\">\n",
        "     <td style=\"vertical-align: middle\">\"bird\"</td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/original_bird.png\" width=200/></td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/canny_map.png\" width=200/></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Similarly, if we were to condition ControlNet with semantic segmentation maps, a training sample would be like so:\n",
        "\n",
        "<table>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <th>Prompt</th>\n",
        "    <th>Original Image</th>\n",
        "    <th>Conditioning</th>\n",
        "</tr>\n",
        "<tr style=\"text-align: center;\">\n",
        "    <td style=\"vertical-align: middle\">\"big house\"</td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/original_house.png\" width=300/></td>\n",
        "    <td><img class=\"mx-auto\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/controlnet/segmentation_map.png\" width=300/></td>\n",
        "</tr>\n",
        "</table>\n",
        "\n",
        "Every new type of conditioning requires training a new copy of ControlNet weights.\n",
        "The paper proposed 8 different conditioning models that are all [supported](https://huggingface.co/lllyasviel?search=controlnet) in Diffusers!\n",
        "\n",
        "For inference, both the pre-trained diffusion models weights as well as the trained ControlNet weights are needed. For example, using [Stable Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5)\n",
        "with a ControlNet checkpoint require roughly 700 million more parameters compared to just using the original Stable Diffusion model, which makes ControlNet a bit more memory-expensive for inference.\n",
        "\n",
        "Because the pre-trained diffusion models are looked during training, one only needs to switch out the ControlNet parameters when using a different conditioning. This makes it fairly simple\n",
        "to deploy multiple ControlNet weights in one application as we will see below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AocAEIA8n33t"
      },
      "source": [
        "# The `StableDiffusionControlNetPipeline`\n",
        "\n",
        "Before we begin, we want to give a huge shout-out to the community contributor [Takuma Mori](https://github.com/takuma104) for having led the integration of ControlNet into Diffusers â¤ï¸ .\n",
        "\n",
        "To experiment with ControlNet, Diffusers exposes the [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet) similar to\n",
        "the [other Diffusers pipelines](https://huggingface.co/docs/diffusers/api/pipelines/overview). Central to the [`StableDiffusionControlNetPipeline`] is the `controlnet` argument which lets us provide a particular trained [`ControlNetModel`](https://huggingface.co/docs/diffusers/main/en/api/models#diffusers.ControlNetModel) instance while keeping the pre-trained diffusion model weights the same.\n",
        "\n",
        "We will explore different use cases with the `StableDiffusionControlNetPipeline` in this blog post. The first ControlNet model we are going to walk through is the [Canny model](https://huggingface.co/runwayml/stable-diffusion-v1-5) - this is one of the most popular models that generated some of the amazing images you are libely seeing on the internet.\n",
        "\n",
        "We welcome you to run the code snippets shown in the sections below with [this Colab Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/controlnet.ipynb).\n",
        "\n",
        "Before we begin, let's make sure we have all the necessary libraries installed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "93w8GFi_mcq2",
        "outputId": "11ad6a7d-20fc-42b5-fd66-25828fafd56b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m737.4/737.4 kB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\n",
            "fastai 2.7.18 requires torch<2.6,>=1.10, but you have torch 2.6.0 which is incompatible.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q diffusers==0.14.0 transformers xformers git+https://github.com/huggingface/accelerate.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XodfZnqkoFc7"
      },
      "source": [
        "To process different conditionings depending on the chosen ControlNet, we also need to install some\n",
        "additional dependencies:\n",
        "- [OpenCV](https://opencv.org/)\n",
        "- [controlnet-aux](https://github.com/patrickvonplaten/controlnet_aux#controlnet-auxiliary-models) - a simple collection of pre-processing models for ControlNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K-3yzE7GoC5l",
        "outputId": "9941b4f1-6ffb-44c7-c286-2584f6b18002",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/282.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m282.4/282.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m510.0/510.0 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xformers 0.0.29.post3 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q opencv-contrib-python\n",
        "!pip install -q controlnet_aux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjPWgbkLoObY"
      },
      "source": [
        "We will use the famous painting [\"Girl With A Pearl\"](https://en.wikipedia.org/wiki/Girl_with_a_Pearl_Earring) for this example. So, let's download the image and take a look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 550
        },
        "id": "DhNdz4rooMLq",
        "outputId": "2f5dffaa-60da-4248-f423-141e068e8aa7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4c08e01a9b2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStableDiffusionControlNetPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdiffusers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m image = load_image(\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"0.14.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m from .utils import (\n\u001b[1;32m      5\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/configuration_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDIFFUSERS_CACHE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDummyObject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdeprecation_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeprecate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdoc_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreplace_example_docstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdynamic_modules_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_class_from_dynamic_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhub_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHF_HUB_OFFLINE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_user_agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m from .import_utils import (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/diffusers/utils/dynamic_modules_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0murllib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_hub_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline\n",
        "from diffusers.utils import load_image\n",
        "\n",
        "image = load_image(\n",
        "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\"\n",
        ")\n",
        "image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPesoRq0oVXn"
      },
      "source": [
        "Next, we will put the image through the canny pre-processor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TU8LP89oSUO"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "image = np.array(image)\n",
        "\n",
        "low_threshold = 100\n",
        "high_threshold = 200\n",
        "\n",
        "image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "image = image[:, :, None]\n",
        "image = np.concatenate([image, image, image], axis=2)\n",
        "canny_image = Image.fromarray(image)\n",
        "canny_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFsPhiz0obBy"
      },
      "source": [
        "As we can see, it is essentially edge detection.\n",
        "\n",
        "Now, we load [runwaylml/stable-diffusion-v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) as well as the [ControlNet model for canny edges](https://huggingface.co/lllyasviel/sd-controlnet-canny). The models are loaded in half-precision (`torch.dtype`) to allow for fast and memory-efficient inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Jnb2HjQoXnH"
      },
      "outputs": [],
      "source": [
        "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel\n",
        "import torch\n",
        "\n",
        "controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g27cB03roi8K"
      },
      "source": [
        "Instead of using Stable Diffusion's default [PNDMScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/pndm), we use one of the currently fastest\n",
        "diffusion model schedulers, called [UniPCMultistepScheduler](https://huggingface.co/docs/diffusers/main/en/api/schedulers/unipc).\n",
        "Choosing an improved scheduler can drastically reduce inference time - in our case we are able to reduce the number of inference steps from 50 to 20 while more or less\n",
        "keeping the same image generation quality. More information regarding schedulers can be found [here](https://huggingface.co/docs/diffusers/main/en/using-diffusers/schedulers)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nA7v3raJogdm"
      },
      "outputs": [],
      "source": [
        "from diffusers import UniPCMultistepScheduler\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oASrhg-_Zj06"
      },
      "source": [
        "Instead of loading our pipeline directly to GPU, we instead enable smart CPU offloading which\n",
        "can be achieved with the [`enable_model_cpu_offload` function](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet#diffusers.StableDiffusionControlNetPipeline.enable_model_cpu_offload).\n",
        "\n",
        "Remember that during inference diffusion models, such as Stable Diffusion require not just one but multiple model components that are run sequentially.\n",
        "In the case of Stable Diffusion with ControlNet, we first use the CLIP text encoder, then the diffusion model unet and control net, then the VAE decoder and finally run a safety checker.\n",
        "Most components are only run once during the diffusion process and are thus not required to occupy GPU memory all the time. By enabling smart model offloading, we make sure\n",
        "that each component is only loaded into GPU when it's needed so that we can significantly save memory consumption without significantly slowing down infenence.\n",
        "\n",
        "**Note**: When running `enable_model_cpu_offload`, do not manually move the pipeline to GPU with `.to(\"cuda\")` - once CPU offloading is enabled, the pipeline automatically takes care of GPU memory management."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z--jWjw-ZhXO"
      },
      "outputs": [],
      "source": [
        "pipe.enable_model_cpu_offload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uurrga1WZnMD"
      },
      "source": [
        "Finally, we want to take full advantage of the amazing [FlashAttention/xformers](https://github.com/facebookresearch/xformers) attention layer acceleration, so let's enable this! If this command does not work for you, you might not have `xformers` correctly installed.\n",
        "In this case, you can just skip the following line of code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXGPAOIBZrG2"
      },
      "outputs": [],
      "source": [
        "pipe.enable_xformers_memory_efficient_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUwSOmbloolZ"
      },
      "source": [
        "Now we are ready to run the ControlNet pipeline!\n",
        "\n",
        "We still provide a prompt to guide the image generation process, just like what we would normally do with a Stable Diffusion image-to-image pipeline. However, ControlNet will allow a lot more control over the generated image because we will be able to control the exact composition in generated image with the canny edge image we just created.\n",
        "\n",
        "It will be fun to see some images where contemporary celebrities posing for this exact same painting from the 17th century. And it's really easy to do that with ControlNet, all we have to do is to include the names of these celebrities in the prompt!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsv55Py8onJx"
      },
      "outputs": [],
      "source": [
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "prompt = \", best quality, extremely detailed\"\n",
        "prompt = [t + prompt for t in [\"Sandra Oh\", \"Kim Kardashian\", \"rihanna\", \"taylor swift\"]]\n",
        "generator = [torch.Generator(device=\"cpu\").manual_seed(2) for i in range(len(prompt))]\n",
        "\n",
        "output = pipe(\n",
        "    prompt,\n",
        "    canny_image,\n",
        "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * len(prompt),\n",
        "    generator=generator,\n",
        "    num_inference_steps=20,\n",
        ")\n",
        "\n",
        "image_grid(output.images, 2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzvLO2QjpDzh"
      },
      "source": [
        "We can effortlessly combine ControlNet combines with fine-tuning too! For example, we can fine-tune a model with [DreamBooth](https://huggingface.co/docs/diffusers/main/en/training/dreambooth), and use it to render ourselves into different scenes.\n",
        "\n",
        "In this post, we are going to use our beloved Mr Potato Head as an example to show how to use ControlNet with DreamBooth.\n",
        "\n",
        "We can use the same ContrlNet, however instead of using the Stable Diffusion 1.5, we are going to load the [Mr Potato Head model](https://huggingface.co/sd-dreambooth-library/mr-potato-head) into our pipeline - Mr Potato Head is a Stable Diffusion model fine-tuned with Mr Potato Head concept using Dreambooth ğŸ¥”\n",
        "\n",
        "Let's run the above commands again, keeping the same `controlnet` though!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G9Jwn2bosti"
      },
      "outputs": [],
      "source": [
        "model_id = \"sd-dreambooth-library/mr-potato-head\"\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reXDKDfIpJJC"
      },
      "source": [
        "Now let's make Mr Potato posing for [Johannes Vermeer](https://en.wikipedia.org/wiki/Johannes_Vermeer)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9chBU9gmpG9t"
      },
      "outputs": [],
      "source": [
        "generator = torch.manual_seed(2)\n",
        "prompt = \"a photo of sks mr potato head, best quality, extremely detailed\"\n",
        "output = pipe(\n",
        "    prompt,\n",
        "    canny_image,\n",
        "    negative_prompt=\"monochrome, lowres, bad anatomy, worst quality, low quality\",\n",
        "    generator=generator,\n",
        "    num_inference_steps=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02SMnvN2pcuH"
      },
      "source": [
        "It is noticeable that Mr Potato Head is not the best candidate but he tried his best and did a pretty good job in capture some of the essence ğŸŸ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzPiaNsFpUTA"
      },
      "outputs": [],
      "source": [
        "output.images[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVVW46uQpiK-"
      },
      "source": [
        "It is noticeable that Mr Potato Head is not the best candidate but he tried his best and did a pretty good job in capture some of the essence ğŸŸ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7Dkvu1SplcW"
      },
      "source": [
        "Another exclusive application of ControlNet is that we can take a pose from one image and reuse it to generate a different image with the exact same pose. So in this next example, we are going to teach superheroes how to do yoga using [Open Pose ControlNet](https://huggingface.co/lllyasviel/sd-controlnet-openpose)!\n",
        "\n",
        "First, we will need to get some images of people doing yoga:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlM0bqeIpjhG"
      },
      "outputs": [],
      "source": [
        "urls = \"yoga1.jpeg\", \"yoga2.jpeg\", \"yoga3.jpeg\", \"yoga4.jpeg\"\n",
        "imgs = [\n",
        "    load_image(\"https://hf.co/datasets/YiYiXu/controlnet-testing/resolve/main/\" + url)\n",
        "    for url in urls\n",
        "]\n",
        "\n",
        "image_grid(imgs, 2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ercghSW3p00F"
      },
      "source": [
        "Now let's extract yoga poses using the OpenPose pre-processors that are handily available via `controlnet_aux`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PVAWgO4p2Kn"
      },
      "outputs": [],
      "source": [
        "from controlnet_aux import OpenposeDetector\n",
        "\n",
        "model = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "\n",
        "poses = [model(img) for img in imgs]\n",
        "image_grid(poses, 2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8EUHIbDp62I"
      },
      "source": [
        "To use these yoga poses to generate new images, let's create a [Open Pose ControlNet](https://huggingface.co/lllyasviel/sd-controlnet-openpose). We will generate some super-hero images but in the yoga poses shown above. Let's go ğŸš€"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJt7kcdNp2s3"
      },
      "outputs": [],
      "source": [
        "controlnet = ControlNetModel.from_pretrained(\n",
        "    \"fusing/stable-diffusion-v1-5-controlnet-openpose\", torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_id,\n",
        "    controlnet=controlnet,\n",
        "    torch_dtype=torch.float16,\n",
        ")\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe.enable_model_cpu_offload()\n",
        "pipe.enable_xformers_memory_efficient_attention()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKHh8l5Qp_Ui"
      },
      "source": [
        "Now it's yoga time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJdvY4mcp-90"
      },
      "outputs": [],
      "source": [
        "generator = [torch.Generator(device=\"cpu\").manual_seed(2) for i in range(4)]\n",
        "prompt = \"super-hero character, best quality, extremely detailed\"\n",
        "output = pipe(\n",
        "    [prompt] * 4,\n",
        "    poses,\n",
        "    negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"] * 4,\n",
        "    generator=generator,\n",
        "    num_inference_steps=20,\n",
        ")\n",
        "image_grid(output.images, 2, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guhEhxioqEcf"
      },
      "source": [
        "Throughout the examples, we explored multiple facets of the [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet) to show how easy and intuitive it is play around with ControlNet via Diffusers. However, we didn't cover all types of conditionings supported by ControlNet. To know more about those, we encourage you to check out the respective model documentation pages:\n",
        "\n",
        "* [lllyasviel/sd-controlnet-depth](https://huggingface.co/lllyasviel/sd-controlnet-depth)\n",
        "* [lllyasviel/sd-controlnet-hed](https://huggingface.co/lllyasviel/sd-controlnet-hed)\n",
        "* [lllyasviel/sd-controlnet-normal](https://huggingface.co/lllyasviel/sd-controlnet-normal)\n",
        "* [lllyasviel/sd-controlnet-scribble](https://huggingface.co/lllyasviel/sd-controlnet-scribble)\n",
        "* [lllyasviel/sd-controlnet-seg](https://huggingface.co/lllyasviel/sd-controlnet-scribble)\n",
        "* [lllyasviel/sd-controlnet-openpose](https://huggingface.co/lllyasviel/sd-controlnet-openpose)\n",
        "* [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-mlsd)\n",
        "* [lllyasviel/sd-controlnet-mlsd](https://huggingface.co/lllyasviel/sd-controlnet-canny)\n",
        "\n",
        "We welcome you to combine these different elements and share your results with [@diffuserslib](https://twitter.com/diffuserslib). Be sure to check out [the Colab Notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/controlnet.ipynb) to take some of the above examples for a spin!\n",
        "\n",
        "We also showed some techniques to make the generation process faster and memory-friendly by using a fast scheduler, smart model offloading and `xformers`. With these techniques combined the generation process should take only ~3 seconds on a V100 GPU and consumes just ~4 GBs of VRAM for a single image âš¡ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMdYxVKaqGeg"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "We have been playing a lot with [`StableDiffusionControlNetPipeline`](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet), and our experience has been fun so far! Weâ€™re excited to see what the community builds on top of this pipeline. If you want to check out other pipelines and techniques supported in Diffusers that allow for controlled generation, check out our [official documentation](https://huggingface.co/docs/diffusers/main/en/using-diffusers/controlling_generation).\n",
        "\n",
        "If you cannot wait to try out ControlNet directly, we got you covered as well! Simply click on one of the following spaces to play around with ControlNet:\n",
        "- [![Canny ControlNet Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/diffusers/controlnet-canny)\n",
        "- [![OpenPose ControlNet Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/diffusers/controlnet-openpose)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuClass": "premium"
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}